# 数据聚合服务

[TOC]

## 关键词

go;redis;MySQL;xml;filebeat

## 系统架构图

![](http://kevinpub.ifree258.top/operation_system/design/数据聚合服架构图v1.0.png)

## What

将 log 文件的日志数据，聚合到 MySQL 数据库。

## Why

将游戏业务记录在 log 中的埋点数据，聚合到数据库中（MySQL 会同步到 ClickHouse），提供给经营分析服务使用。

## How

### 解析 ov_log.xml 文件

有数据库结构描述文件 `ov_log.xml` 格式如下：

```xml
<metalib tagsetversion="1" name="TLog" version="1">
  <struct name="STRUCT_1">
    <entry name="ENTRY_1"/>
  </struct>

  <struct name="STRUCT_2">
    <entry name="ENTRY_1"/>
    <entry name="ENTRY_2"/>
  </struct>

   <!-- many structs -->

  <union name="UNION" version="1">
    <entry type="STRUCT_1"       id="ID_STRUCT_1" />
    <entry type="STRUCT_2"       id="ID_STRUCT_2" />
  </union>
</metalib>
```

需解析得到

`map[表名]=[字段名1, 字段名2]`映射关系`map["STRUCT_1"] == ["ENTRY_1"], map["STRUCT_2"] == ["ENTRY_1","ENTRY_2"]`

和

`map[表名ID]=表名`映射关系`map2["ID_STRUCT_1"] == "STRUCT_1", map2["ID_STRUCT_2"] == "STRUCT_2"`

#### 实现方案

go 语言实现 xml 解析程序。

#### go 程序

```go
// cmd/dataagg-server/xml2map.go
package main

import (
	"encoding/xml"
	"os"
	"strings"

	"github.com/pkg/errors"

	"github.com/go-kratos/kratos/v2/log"
)

const OV_LOG_PATH = "../../db/ov_log.xml"

var (
	MapStructEntry map[string][]string
	MapIdType      map[string]string
)

// xml 结构
type Entry struct {
	Name string `xml:"name,attr"`
	Type string `xml:"type,attr"`
	Id   string `xml:"id,attr"`
}
type Struct struct {
	Name  string  `xml:"name,attr"`
	Entry []Entry `xml:"entry"`
}
type Union struct {
	Name  string  `xml:"name,attr"`
	Entry []Entry `xml:"entry"`
}
type metalib struct {
	XMLName xml.Name `xml:"metalib"`
	Struct  []Struct `xml:"struct"`
	Union   Union    `xml:"union"`
}

// 解析 ov_log.xml 表结构，获取 mapStruct, mapUnion
func XML2Map() error {
	var err error
	filePath := OV_LOG_PATH

	fi, err := os.Stat(filePath)
	if err != nil {
		return errors.Wrap(err, "Stat Failed")
	}
	fh, err := os.Open(filePath)
	if err != nil {
		return errors.Wrap(err, "Open Failed")
	}
	defer fh.Close()
	buf := make([]byte, fi.Size())
	_, err = fh.Read(buf)
	if err != nil {
		return errors.Wrap(err, "Read Failed")
	}
	metalib := metalib{}
	err = xml.Unmarshal(buf, &metalib)
	if err != nil {
		return errors.Wrap(err, "Unmarshal Failed")
	}

	MapStructEntry = make(map[string][]string)
	for _, stStruct := range metalib.Struct {
		sliceEntryName := make([]string, 0)
		for _, stEntry := range stStruct.Entry {
			// fixed: if type is "LOGDT*", the name should be LOGDT's entry.name
			if strings.Contains(stEntry.Type, "LOGDT") {
				LOGDTNames := MapStructEntry[stEntry.Type]
				sliceEntryName = append(sliceEntryName, LOGDTNames...)
				continue
			}
			sliceEntryName = append(sliceEntryName, stEntry.Name)
		}
		MapStructEntry[stStruct.Name] = sliceEntryName
	}

	MapIdType = make(map[string]string)
	for _, stEntry := range metalib.Union.Entry {
		MapIdType[stEntry.Id] = stEntry.Type
	}
	log.Infof("================XML2Map Successed！==================")
	return nil
}
```

### 采集 log 日志文件的数据

业务服将埋点数据记录到`/data/home/user00/sgame/log`目录下的文件中，其中经营分析日志在`gamesvr_oss.log`中。

日志格式为：`LOGID_ITEM_CHANGE|1.0.12.1|2022-08-17 14:29:35||-1|1001|3173432536676033|1|30|3173432536676033|0|4|2503|10|30|1|0|0|0`
说明：表名|字段值 1|字段值 2|...

#### 实现方案

使用 [filebeat](https://www.elastic.co/cn/beats/filebeat) 实现，filebeat 是一个轻量型日志采集器， filebeat 进程实时 tail -f 日志，然后将日志数据转发到 redis 队列，再由一个自定义进程实时消费 redis 队列数据入库到 MySQL。

#### filebeat 的安装和配置

详细可参看[官方文档](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html)

##### 安装

```powershell
curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.3.3-linux-x86_64.tar.gz
tar xzvf filebeat-8.3.3-linux-x86_64.tar.gz
```

##### 配置

```powershell
vim /etc/filebeat/filebeat.yml
```

主要配置内容如下

```powershell
# ============================== Filebeat inputs ===============================
filebeat.inputs:
- type: filestream
  id: ali1-filestream-id
  enabled: true
  paths:
    - /data/home/user00/sgame/log/gamesvr_oss.log

# ================================== Outputs ===================================
output.redis:
  hosts: ["10.4.1.29"]
  password: "xxx"
  key: "filebeat"
  db: 2
  timeout: 5
```

说明：

- enable 改为 true (启用此配置)
- paths 为要采集的目录的路径
- output 设为 redis

> filebeat 是使用 RPUSH 发送到 redis list

##### 启动 filebeat

```powershell
systemctl start filebeat
```

### 将 log 数据存储到 MySQL

上一步已经将 log 数据采集到 redis 中，接下来需要将 redis 数据落地到 MySQL。

#### 实现方案

启动一个自定义进程，通过死循环的方式，先判断 redis 队列长度，如果长度为 0 则等待 60s；如果队列中有数据，则使用 redis pipeline 操作读取 100 条数据执行入库处理。

#### go 程序

`cmd/dataagg-server/handle.go`

```go
// Handle 处理 Redis 数据，落地到 MySQL
func Handle() {
	log.Infof("================Enter Handle================")
	for {
		if Data.RedisCli.LLen(Ctx, data.Redis_Key).Val() ==0 {
			log.Infof("redis log-oss list has not Data.")
			time.Sleep(60*time.Second)
		}else {
			RedisPipeline_LogOss(100)
		}
	}
}

// 从 Redis 读数据批量处理，n:处理数量
func RedisPipeline_LogOss(n int64) {
	pipe := Data.RedisCli.Pipeline()
	// LRange(s,e) 获取[s,d]的数据。(0,99)表示获取前100条
	// LTrim(s,e) 保留[s,e]的数据，其他删除。(100,-1)表示删除前100条
	stringSliceCmd := pipe.LRange(Ctx, data.Redis_Key, 0, n-1)
	pipe.LTrim(Ctx, data.Redis_Key, n, -1)
	// pipeline 一次执行多条命令，节省网络往返时间(RTT)
	_, err := pipe.Exec(Ctx)
	if err != nil {
		log.Errorf("Exec err: %v", err)
        return
	}
	HandleMessage(stringSliceCmd)
}

// 测试接收 stringSliceCmd，处理 message，插入 MySQL
func HandleMessage(stringSliceCmd *redis.StringSliceCmd) error {
	for _, value := range stringSliceCmd.Val() {
		var raw map[string]interface{}
		if err := json.Unmarshal([]byte(value), &raw); err != nil {
			log.Errorf("Unmarshal value: %v, err: %v\n", value, err)
			// 如果错误，将此数据 push 回 Redis
			Data.RedisCli.RPush(Ctx, data.Redis_Key, value)
			continue
		}
		// 获得 message
		message := raw["message"]
		stringMessage := fmt.Sprint(message)
		sliceMessage := strings.Split(stringMessage, "|")
		tableID := sliceMessage[0]
		// 删除 sliceMessage 第一个元素表名，为插入表的各列数据
		datas := sliceMessage[1:]
		// 通过 ov_log.xml 生成的 map 获取表名、列名
		tableName := MapIdType[tableID]
		tableColumns := MapStructEntry[tableName]
		// fixed: log 和 xml 表字段数不一致，自动截取，以xml定义的字段数量为准
		if len(datas) != len(tableColumns) {
			datas = datas[0:len(tableColumns)]
		}
		//  fixed: data too long for column
		for i, d := range datas {
			if len(d) > data.Max_Column_Length {
				// 如果字符串长度超过191个字符，自动截取[0:191]
				datas[i] = d[0:data.Max_Column_Length]
			}
		}
		// 转化为 SQL 的格式
		strDatas := FormatSlice2String(datas, "'")
		strTableColumns := FormatSlice2String(tableColumns, "`")

		// 根据表名、ov_log.xml解析的表结构将 message 数据插入到表中
		// 需指定字段，或插入所有字段值，否则 Error 1136 - Column count doesn't match value count at row 1
		// INSERT INTO LOGDT_ITEM_CHANGE(`GameSvrId`, `dtEventTime`, `vGameAppid`, `PlatID`, `iZoneAreaID`, `vopenid`, `Level`, `PvpLevel`, `GarenaOpenID`, `ChgType`, `ChgItemType`, `ChgItemID`, `ChgItemCnt`, `CurItemCnt`, `ChgEvent`, `EventParam1`, `EventParam2`, `EventParam3`) VALUES ('1.0.12.1', '2022-06-10 11:14:25', '', '-1', '1001', '1516006973646350', 1, 30, '1516006973646350', 0, 4, 1501, 10, 530, 1, 0, 0, 0);
		strSQLInsertLog := fmt.Sprintf("INSERT INTO %s%s VALUES %s", tableName, strTableColumns, strDatas)

		err := Data.GormDB.Exec(strSQLInsertLog).Error
		if err != nil {
			log.Errorf("INSERT value:%v, err: %v\n", value, err)
			Data.RedisCli.RPush(Ctx, data.Redis_Key, value)
			continue
		}
		log.Infof("INSERT Success! Table: %v, Datas: %v\n", tableName, strDatas)
	}
	return nil
}

// FormatSlice2String concatenates the elements of its first argument to create a single string.
// The separator string sep is placed encase elements in the resulting string.
// Eg. when elems=["a","b","c"], sep=`; then result="(`a`,`b`,`c`)"
func FormatSlice2String(elems []string, sep string) string {
	var temp = make([]string, len(elems))
	for i, s := range elems {
		temp[i] = fmt.Sprintf("%s%s%s", sep, s, sep)
	}
	var result = "(" + strings.Join(temp, ",") + ")"
	return result
}
```

## QA

### Redis 数据落地 MySQL 效率

说明：从 Redis 将数据落地到 MySQL，如果用 `redis.pop()` 每弹出 1 条数据都要连接一次 Redis 服务器，当数据量大的时候，实际上超过一半的时间都消耗在了网络请求上面。

所以使用另一种方案， `range + trim` 来实现，用 `LRange` 来获取数据， `LTrim` 来删除数据，这样就能一次弹出多条数据。

但是直接这样用还有个问题：由于获取数据与删除数据是两条命令，中间有时间差。这就导致在多个线程或者进程同时执行这两条代码的时候，出现竞争。也就是进程 1 刚刚获取了前 5000 条数据，然后进程 2 同样获取这 5000 条数据，然后进程 1 删除前 5000 条数据，然后进程 2 再删除 5000 条数据。

**这样一来，两个进程获取了相同的 5000 条数据，但是却删了 10000 条数据。**

为了解决这个问题，必须让获取数据与删除数据这两个操作变成一个“**原子操作**”。所谓的原子操作就是只一个最小的操作单位，它不会被中途打断。

要解决这个问题，我们就需要使用 Redis 的 `pipeline` 功能。它可以把多条命令放在一个网络请求中发送到服务器，并默认在一个事务中执行这些命令。一个事务是不会被打断的，从事务开始然后执行里面的多个命令到结束的整个过程，可以看做一个原子操作。

`pipeline` 的使用方法如下：

```go
	pipe := Data.RedisCli.Pipeline()
	// LRange(s,e) 获取[s,d]的数据。(0,99)表示获取前100条
	// LTrim(s,e) 保留[s,e]的数据，其他删除。(100,-1)表示删除前100条
	stringSliceCmd := pipe.LRange(Ctx, data.Redis_Key, 0, n-1)
	pipe.LTrim(Ctx, data.Redis_Key, n, -1)
	// pipeline 一次执行多条命令，节省网络往返时间(RTT)
	_, err := pipe.Exec(Ctx)
```

入口通过一个死循环执行代码，判断如果 Redis 队列中没有数据则等待，有数据则获取并删除数据进行入库处理。

```go
// Handle 处理 Redis 数据，落地到 MySQL
func Handle() {
	Data.Log.Infof("================Enter Handle================")
	for {
		if Data.RedisCli.LLen(Ctx, data.Redis_Key).Val() == 0 {
			Data.Log.Infof("redis log-oss list has not Data.")
			time.Sleep(60 * time.Second)
		} else {
			RedisPipeline_LogOss(100)
		}
	}
}

// 从 Redis 读数据批量处理，n:处理数量
func RedisPipeline_LogOss(n int64) {
	pipe := Data.RedisCli.Pipeline()
	// LRange(s,e) 获取[s,d]的数据。(0,99)表示获取前100条
	// LTrim(s,e) 保留[s,e]的数据，其他删除。(100,-1)表示删除前100条
	stringSliceCmd := pipe.LRange(Ctx, data.Redis_Key, 0, n-1)
	pipe.LTrim(Ctx, data.Redis_Key, n, -1)
	// pipeline 一次执行多条命令，节省网络往返时间(RTT)
	_, err := pipe.Exec(Ctx)
	if err != nil {
		Data.Log.Errorf("Exec err: %v", err)
		return
	}
	// 接收 stringSliceCmd，处理 message，插入 MySQL
	HandleMessage(stringSliceCmd)
}
```

### Redis 数据落地 MySQL 完整性

如果执行异常则将数据 `RPush` 回 Redis List，以此来保障数据的完整性。

```go
// 接收 stringSliceCmd，处理 message，插入 MySQL
func HandleMessage(stringSliceCmd *redis.StringSliceCmd) error {
	for _, value := range stringSliceCmd.Val() {
		var raw map[string]interface{}
		if err := json.Unmarshal([]byte(value), &raw); err != nil {
			Data.Log.Errorf("Unmarshal value: %v, err: %v\n", value, err)
			// 如果错误，将此数据 push 回 Redis
			Data.RedisCli.RPush(Ctx, data.Redis_Key, value)
			continue
			// return errors.Wrap(err, "Unmarshal err")
		}
		// 获得 message
		message := raw["message"]
		stringMessage := fmt.Sprint(message)
		sliceMessage := strings.Split(stringMessage, "|")
		tableID := sliceMessage[0]
		// 删除 sliceMessage 第一个元素表名，为插入表的各列数据
		datas := sliceMessage[1:]
		// 通过 ov_log.xml 生成的 map 获取表名、列名
		tableName := MapIdType[tableID]
		tableColumns := MapStructEntry[tableName]
		// fixed: log 和 xml 表字段数不一致，自动截取，以xml定义的字段数量为准
		if len(datas) != len(tableColumns) {
			datas = datas[0:len(tableColumns)]
		}
		//  fixed: data too long for column
		for i, d := range datas {
			if len(d) > data.Max_Column_Length {
				// 如果字符串长度超过191个字符，自动截取[0:191]
				datas[i] = d[0:data.Max_Column_Length]
			}
		}
		// 转化为 SQL 的格式
		strDatas := FormatSlice2String(datas, "'")
		strTableColumns := FormatSlice2String(tableColumns, "`")

		// 根据表名、ov_log.xml解析的表结构将 message 数据插入到表中
		// 需指定字段，或插入所有字段值，否则 Error 1136 - Column count doesn't match value count at row 1
		// INSERT INTO LOGDT_ITEM_CHANGE(`GameSvrId`, `dtEventTime`, `vGameAppid`, `PlatID`, `iZoneAreaID`, `vopenid`, `Level`, `PvpLevel`, `GarenaOpenID`, `ChgType`, `ChgItemType`, `ChgItemID`, `ChgItemCnt`, `CurItemCnt`, `ChgEvent`, `EventParam1`, `EventParam2`, `EventParam3`) VALUES ('1.0.12.1', '2022-06-10 11:14:25', '', '-1', '1001', '1516006973646350', 1, 30, '1516006973646350', 0, 4, 1501, 10, 530, 1, 0, 0, 0);
		strSQLInsertLog := fmt.Sprintf("INSERT INTO %s%s VALUES %s", tableName, strTableColumns, strDatas)

		err := Data.GormDB.Exec(strSQLInsertLog).Error
		if err != nil {
			Data.Log.Errorf("INSERT value: %v, err: %v\n", value, err)
			Data.RedisCli.RPush(Ctx, data.Redis_Key, value)
			continue
		}
		Data.Log.Infof("INSERT Success! Table: %v, Datas: %v\n", tableName, strDatas)
	}
	return nil
}
```

## 参考

[一日一技：如何从 Redis 的列表中一次性 pop 多条数据？](https://cloud.tencent.com/developer/article/1554080)

[Go 语言操作 Redis](https://www.liwenzhou.com/posts/Go/go_redis/#autoid-2-4-5)

[Go 操作 Redis:go-redis 库使用指南](https://www.modb.pro/db/404620)
